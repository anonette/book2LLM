{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the number of words to the number of tokens in a paragraph.  \n",
    "\n",
    "The number of words is typically less than the number of tokens because a tokenizer usually splits on spaces (like a word count would) but also splits punctuation and special characters into separate tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_and_tokens(filename):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        paragraphs = []\n",
    "        paragraph = []\n",
    "        for line in file:\n",
    "            if line.startswith(\"  \"):\n",
    "                paragraph.append(line.strip())\n",
    "            elif paragraph:\n",
    "                paragraphs.append(\" \".join(paragraph))\n",
    "                paragraph = []\n",
    "        if paragraph:\n",
    "            paragraphs.append(\" \".join(paragraph))\n",
    "\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            words = len(paragraph.split())\n",
    "            tokens = len(tokenizer.tokenize(paragraph))\n",
    "            print(f\"Paragraph {i+1}: {words} words, {tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1: 676 words, 862 tokens\n",
      "Paragraph 2: 781 words, 1015 tokens\n",
      "Paragraph 3: 615 words, 865 tokens\n",
      "Paragraph 4: 687 words, 1006 tokens\n",
      "Paragraph 5: 534 words, 704 tokens\n",
      "Paragraph 6: 660 words, 856 tokens\n",
      "Paragraph 7: 692 words, 917 tokens\n",
      "Paragraph 8: 872 words, 1211 tokens\n",
      "Paragraph 9: 477 words, 644 tokens\n",
      "Paragraph 10: 621 words, 819 tokens\n",
      "Paragraph 11: 889 words, 1173 tokens\n",
      "Paragraph 12: 510 words, 756 tokens\n",
      "Paragraph 13: 1058 words, 1430 tokens\n",
      "Paragraph 14: 826 words, 1219 tokens\n",
      "Paragraph 15: 717 words, 928 tokens\n",
      "Paragraph 16: 785 words, 1056 tokens\n",
      "Paragraph 17: 935 words, 1263 tokens\n",
      "Paragraph 18: 908 words, 1176 tokens\n",
      "Paragraph 19: 640 words, 860 tokens\n",
      "Paragraph 20: 995 words, 1430 tokens\n",
      "Paragraph 21: 554 words, 732 tokens\n",
      "Paragraph 22: 855 words, 1166 tokens\n",
      "Paragraph 23: 683 words, 935 tokens\n",
      "Paragraph 24: 710 words, 949 tokens\n",
      "Paragraph 25: 759 words, 998 tokens\n",
      "Paragraph 26: 1193 words, 1566 tokens\n",
      "Paragraph 27: 851 words, 1117 tokens\n",
      "Paragraph 28: 788 words, 1037 tokens\n",
      "Paragraph 29: 803 words, 1111 tokens\n",
      "Paragraph 30: 585 words, 840 tokens\n",
      "Paragraph 31: 706 words, 958 tokens\n",
      "Paragraph 32: 817 words, 1120 tokens\n",
      "Paragraph 33: 900 words, 1251 tokens\n",
      "Paragraph 34: 850 words, 1135 tokens\n",
      "Paragraph 35: 895 words, 1243 tokens\n",
      "Paragraph 36: 885 words, 1214 tokens\n",
      "Paragraph 37: 837 words, 1213 tokens\n",
      "Paragraph 38: 679 words, 974 tokens\n",
      "Paragraph 39: 1075 words, 1498 tokens\n",
      "Paragraph 40: 765 words, 1019 tokens\n",
      "Paragraph 41: 1017 words, 1384 tokens\n",
      "Paragraph 42: 707 words, 994 tokens\n",
      "Paragraph 43: 1243 words, 1681 tokens\n",
      "Paragraph 44: 723 words, 1035 tokens\n",
      "Paragraph 45: 588 words, 806 tokens\n",
      "Paragraph 46: 972 words, 1296 tokens\n",
      "Paragraph 47: 329 words, 426 tokens\n",
      "Paragraph 48: 466 words, 573 tokens\n",
      "Paragraph 49: 676 words, 921 tokens\n",
      "Paragraph 50: 455 words, 631 tokens\n",
      "Paragraph 51: 1170 words, 1628 tokens\n",
      "Paragraph 52: 970 words, 1255 tokens\n",
      "Paragraph 53: 525 words, 698 tokens\n",
      "Paragraph 54: 519 words, 721 tokens\n",
      "Paragraph 55: 602 words, 849 tokens\n",
      "Paragraph 56: 989 words, 1367 tokens\n",
      "Paragraph 57: 686 words, 905 tokens\n",
      "Paragraph 58: 465 words, 684 tokens\n",
      "Paragraph 59: 700 words, 923 tokens\n",
      "Paragraph 60: 436 words, 592 tokens\n",
      "Paragraph 61: 701 words, 926 tokens\n",
      "Paragraph 62: 1015 words, 1408 tokens\n",
      "Paragraph 63: 919 words, 1289 tokens\n",
      "Paragraph 64: 944 words, 1247 tokens\n",
      "Paragraph 65: 894 words, 1310 tokens\n",
      "Paragraph 66: 658 words, 848 tokens\n",
      "Paragraph 67: 875 words, 1188 tokens\n",
      "Paragraph 68: 817 words, 1045 tokens\n",
      "Paragraph 69: 627 words, 895 tokens\n",
      "Paragraph 70: 1238 words, 1733 tokens\n",
      "Paragraph 71: 584 words, 837 tokens\n",
      "Paragraph 72: 667 words, 885 tokens\n",
      "Paragraph 73: 1449 words, 1910 tokens\n",
      "Paragraph 74: 1017 words, 1327 tokens\n",
      "Paragraph 75: 656 words, 870 tokens\n",
      "Paragraph 76: 685 words, 904 tokens\n",
      "Paragraph 77: 517 words, 699 tokens\n",
      "Paragraph 78: 799 words, 1041 tokens\n",
      "Paragraph 79: 893 words, 1256 tokens\n",
      "Paragraph 80: 1666 words, 2169 tokens\n",
      "Paragraph 81: 864 words, 1181 tokens\n",
      "Paragraph 82: 521 words, 756 tokens\n",
      "Paragraph 83: 699 words, 912 tokens\n",
      "Paragraph 84: 928 words, 1244 tokens\n",
      "Paragraph 85: 1183 words, 1602 tokens\n",
      "Paragraph 86: 879 words, 1128 tokens\n",
      "Paragraph 87: 571 words, 788 tokens\n",
      "Paragraph 88: 844 words, 1058 tokens\n",
      "Paragraph 89: 517 words, 695 tokens\n",
      "Paragraph 90: 1002 words, 1255 tokens\n",
      "Paragraph 91: 1021 words, 1375 tokens\n",
      "Paragraph 92: 864 words, 1127 tokens\n",
      "Paragraph 93: 400 words, 506 tokens\n",
      "Paragraph 94: 643 words, 850 tokens\n",
      "Paragraph 95: 926 words, 1159 tokens\n",
      "Paragraph 96: 615 words, 773 tokens\n",
      "Paragraph 97: 756 words, 981 tokens\n",
      "Paragraph 98: 454 words, 766 tokens\n",
      "Paragraph 99: 612 words, 742 tokens\n",
      "Paragraph 100: 690 words, 905 tokens\n",
      "Paragraph 101: 775 words, 974 tokens\n",
      "Paragraph 102: 561 words, 752 tokens\n",
      "Paragraph 103: 894 words, 1161 tokens\n",
      "Paragraph 104: 2029 words, 2658 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = 'data\\BookFinal2023_noref.txt'  # replace with your file name\n",
    "count_words_and_tokens(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
