{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process data\n",
    "\n",
    "With permission of the author, we will demonstrate how to quary the book [myths of automations](https://anonette.net), written by Denisa Kera\n",
    "\n",
    "To achieve this, we will first split the book into chunks, each roughly a page long, then ask fr summary of each pragaraph and suggest three questions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json, warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\BookFinal2023_noref.txt\", \"r\", encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Count the tokens in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from transformers import GPT2Tokenizer\n",
    "# OpenAI GPT-2 tokenizer is the same as GPT-3 tokenizer\n",
    "# we use it to count the number of tokens in the text\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "chunks = text.split('  ')\n",
    "ntokens = []\n",
    "for chunk in chunks:\n",
    "    ntokens.append(len(tokenizer.encode(chunk)))\n",
    "max(ntokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data to chunks, but dont split in middle of paragraphs and try to balances token number with number of paragraph in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Myth of automation\n",
      "\n",
      "1.0 Rituals, Instruments, and Prototypes \n",
      "   Algorithms and data in models and ledgers enact an old fantasy of a future governed by rituals that become instruments, machines, and infrastructures. It is a fantasy of time control and automation that work as a deus ex machina or devil's bridge, offering miracles that turn into curses. Examples go back to the 'predictive analytics' with olive presses in the 6th century BCE, the complaints about the merciless water clock in the 4th century BCE, and Plautus' famous curse of the sundial (Chapter 2). These classic loci show how the fantasies of automation and control over time quickly turn into anxieties about bias, precarity, loss of agency, and sovereignty. \n",
      "\n",
      " Dreams and fears of automation emerge with every new instrument and infrastructure. From the early calendars and clocks to today's reputation and scoring systems, predictive AI, or smart contracts on trustless blockchain ledgers, automation promises a frictionless, evidence-based, and politically neutral future and governance. In the present, the control of time and the future even intensified thanks to the computer clocks that do not measure but generate the signals and cycles needed to synchronize the data on the computers and networks and, by proxy, all the processes in society. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def split_text_keep_format(text, delimiter, split_count):\n",
    "    chunks = []\n",
    "    count = 0\n",
    "    start = 0\n",
    "\n",
    "    # Iterate over the characters in the text\n",
    "    for i in range(len(text) - len(delimiter) + 1):\n",
    "        # If the current slice of the text equals the delimiter\n",
    "        if text[i:i+len(delimiter)] == delimiter:\n",
    "            count += 1\n",
    "            # If the count is equal to the split count\n",
    "            if count == split_count:\n",
    "                # Append the current chunk to the list\n",
    "                chunks.append(text[start:i])\n",
    "                # Reset the start and count\n",
    "                start = i + len(delimiter)\n",
    "                count = 0\n",
    "\n",
    "    # Append the last chunk to the list\n",
    "    chunks.append(text[start:])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text_keep_format(text, '  ', 3)\n",
    "\n",
    "\n",
    "ntokens = []\n",
    "for chunk in chunks:\n",
    "    ntokens.append(len(tokenizer.encode(chunk)))\n",
    "    # print (len(tokenizer.encode(chunk)))\n",
    "\n",
    "max_tokens = max(ntokens)\n",
    "print (chunks[0])\n",
    "print (chunks[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1500 tokens. The model we will use is text-davinci-002, which has a limit of 4096 tokens, so we don't need to worry about breaking the chunks down further.\n",
    "\n",
    "We will group the shorter chunks into chunks of around 1000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_chunks(chunks, ntokens, max_len=1777, hard_max_len=3000):\n",
    "    \"\"\"\n",
    "    Group very short chunks, to form approximately page long chunks.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    cur_batch = \"\"\n",
    "    cur_tokens = 0\n",
    "    \n",
    "    # iterate over chunks, and group the short ones together\n",
    "    for chunk, ntoken in zip(chunks, ntokens):\n",
    "        # discard chunks that exceed hard max length\n",
    "        if ntoken > hard_max_len:\n",
    "            print(f\"Warning: Chunk discarded for being too long ({ntoken} tokens > {hard_max_len} token limit). Preview: '{chunk[:50]}...'\")\n",
    "            continue\n",
    "\n",
    "        # if room in current batch, add new chunk\n",
    "        if cur_tokens + 1 + ntoken <= max_len:\n",
    "            cur_batch += \"\\n\\n\" + chunk\n",
    "            cur_tokens += 1 + ntoken  # adds 1 token for the two newlines\n",
    "        # otherwise, record the batch and start a new one\n",
    "        else:\n",
    "            batches.append(cur_batch)\n",
    "            cur_batch = chunk\n",
    "            cur_tokens = ntoken\n",
    "            \n",
    "    if cur_batch:  # add the last batch if it's not empty\n",
    "        batches.append(cur_batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "\n",
    "chunks = group_chunks(chunks, ntokens)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "﻿1. Myth of automation\n",
      "\n",
      "1.0 Rituals, Instruments, and Prototypes \n",
      "   Algorithms and data in models and ledgers enact an old fantasy of a future governed by rituals that become instruments, machines, and infrastructures. It is a fantasy of time control and automation that work as a deus ex machina or devil's bridge, offering miracles that turn into curses. Examples go back to the 'predictive analytics' with olive presses in the 6th century BCE, the complaints about the merciless water clock in the 4th century BCE, and Plautus' famous curse of the sundial (Chapter 2). These classic loci show how the fantasies of automation and control over time quickly turn into anxieties about bias, precarity, loss of agency, and sovereignty. \n",
      "\n",
      "\n",
      " Dreams and fears of automation emerge with every new instrument and infrastructure. From the early calendars and clocks to today's reputation and scoring systems, predictive AI, or smart contracts on trustless blockchain ledgers, automation promises a frictionless, evidence-based, and politically neutral future and governance. In the present, the control of time and the future even intensified thanks to the computer clocks that do not measure but generate the signals and cycles needed to synchronize the data on the computers and networks and, by proxy, all the processes in society. \n",
      " \n",
      "\n",
      "The pervasive control of time through algorithms and associated bureaucratic structures, such as standards, further erodes the experience of time as an agency to discover, decide, disrupt, or negotiate the future with others. We will discuss this form of control and the ideal of governance as a myth of automation. To understand where the promise of an ultimate control over the future and time comes from, we will begin with a question. How do matters of public life become rituals and automated services that leave no room for chance, improvisation, but also for agency and action? What are the metaphysical, political, and material conditions under which governance becomes a matter of automation and algorithms? \n",
      "   The reflections on algorithmic governance provide some guidance, but they often focus on the regulation and ownership of the algorithms and data responsible for the automation (Micheli et al. 2020; Elkin-Koren 2020; Saurwein, Just and Latzer 2015; Larsson 2018; Shorey and Howard 2016). Research on algorithmic governance often discusses broad epistemological questions of objectivity and accuracy, including the social, political, and economic implications of algorithms and data in critical data studies (Boyd and Crawford 2012; Ananny and Crawford 2018; Kitchin 2017), even issues of energy and mineral consumption (Crawford 2013; Parikka 2015). These insights will inform our critique, but the focus will remain on uses of the future and time as conditions of automation. We define automation as the control of time through rituals and prototypes of machines and infrastructures that close the future by imposing some type of teleology with universal and final goals.\n",
      "\n",
      "\n",
      " To discuss the problem of automation in governance, we will combine metaphysical questions of time and agency with political questions of power (Chapter 2). What is good governance in an age of prototypes that disrupt and control citizens' public and even private lives, often without clear mandates? How do we ensure that any immense power over the future and time respects any values we hold dear in our institutions and political processes, such as human rights, the rule of law, the values of political pluralism, legitimacy, or accountability?\n",
      " \n",
      "\n",
      "We will discuss these questions by focusing on the relationship between science, technology, and society as a problem of prototyping (Chapters 3 and 4). The dichotomies of agency and structure, or action and knowledge, experiences of time as Chronos and Kairos (timekeeping and timing), will help us understand how prototype defines governance. Rather than prototypes that reduce governance to better automation (Chapter 2), we are interested in prototypes that democratize power over the future and time (Chapters 3 and 4). Prototyping has the potential to open the future to public participation and define political and technological representation as a matter of practice. We will define prototyping that tries to \"save\" time as a public good and reject the myth of automation and disruptive infrastructures as our modern-day devil's bridges (Chapter 4).\n",
      "Genealogy of future making\n",
      "   The discussion of time and the future as a problem of prototyping and automation pays homage to Siegfried Zielinski's project of 'time media' and 'time machines'. We have extended his central question \"Who owns time?\" (Zielinski 2006) into a challenge: How can we make time a public resource that involves citizens in shaping the future and allows them to experience their agency? Rather than examining the forgotten and neglected futures of the past, we will focus on the dominant instruments of time and future control described in Chapter 2 as governance machines. While, as governance machines, prototypes reduce governance to automation, as exploratory tools, they have the potential to support public engagement and represent public interests. The prototypes then become a form of social action through which we can experience our personal and collective power over the future and time.\n",
      "\n",
      "\n",
      " Before science instruments become governance machines embedded in technical standards, market expectations, and other bureaucratic structures (Sections 2.0, 2.1 and 2.2), they are prototypes that support personal and community commitments. Liminal and experimental environments for prototyping in the hackerspaces described in Chapter 3 will offer examples of such engagements with instruments outside the market forces, industry standards, patents, or other power structures. Together with the historical examples or early instruments in Chapter 2, these prototypes will demonstrate the alternatives to governance as automation. In Chapter 4, we will extrapolate these lessons and examples into a proposal for an experimental sandbox that combines prototyping with governance to support personal and social action and agency over the future. \n",
      " \n",
      "\n",
      "The genealogy of automation in Chapter 2 follows Zielinski's revision of media history, but not his goal of resurrecting the ‘dead’ and forgotten futures of the past through art and media archaeology. We capture the dynamic of ‘imagination and calculation’ (Zielinski 2006) as a question of action and structure (knowledge) and focus on how we can democratize future making and prototyping. The early attempts to govern via prototypes (Chapter 2, especially Sections 2.3 and 2.4) will serve as material for our genealogy that aims to ‘exorcise’ automation from governance (and prototypes) in the rest of Chapters 2 and 3.  \n",
      " \n",
      "\n",
      "We call this process exorcism, because it uses genealogy to identify and defuse the arbitrary exercise of power in promissory infrastructures and new instruments that ‘possess’ our private and public lives. By identifying moments of resignation, in relation to politics, agency, and the future, we create an opportunity to rethinking the role of prototyping in governance (Chapters 3 and 4). The alternative history of instrument making from the Renaissance period (Sections 2.3 and 2.4) to today's hackerspaces, makerspaces, and citizen science labs (Chapter 3) illustrates the existence and importance of alternative engagements with instruments and future-making.\n",
      "   To explore prototypes that support agency over the future rather than control, we propose exploratory sandboxes (Chapter 4) that extend the practices identified in the forgotten history of instrument making (Chapter 2) and observed in the liminal spaces (Chapter 3). Before instruments or algorithms become infrastructures owned and regulated by anyone, they are prototypes and explorations that express conflicting ideas, values, and visions for society. At this early, ambiguous, and experimental stage, we need to increase participation and redefine representation. Rather than merely aligning and embedding values and political goals, or propagating technological futures and educating the public about hegemonic infrastructures, the liminal environments of hackerspaces and sandboxes support direct public participation and agency.\n",
      "\n",
      "\n",
      " Liminal and exploratory prototyping enables citizens to define their stake in the future and prevent excesses of algorithmic rule and automation. As stakeholders in the future, citizens understand that time is a common good and a public resource that must be cared for. Instead of being mere users of future infrastructures or media archaeologists lamenting the loss of forgotten futures, citizens express their agency and vision through prototypes. They negotiate their future with others without closing it or claiming that one ideal would serve all. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''you are a text to json converter. you are given a text with following rules\n",
    "                * titles start with no space and ends with newline\n",
    "                * paragraphs are lines starting with two spaces and end with two spaces\n",
    "                add each paragraph, unmodified and uncut, to a list of paragraphs\n",
    "                add summary using the words of the author to each paragraph\n",
    "                add three questions to each paragraph , in the voice of a layman \n",
    "                return response as valid JSONL.\n",
    "                '''\n",
    "\n",
    "\n",
    "def chat_oracle_json(chunk, prompt=prompt, indexc=0):\n",
    "    # Format the index as a string with 4 digits, adding leading zeros as needed\n",
    "    index_str = str(indexc).zfill(4)\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = os.path.join('chunks/', f'chunk_{index_str}.json')\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.isfile(file_path):\n",
    "\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0.77,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": chunk,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        response=completion.choices[0].message\n",
    "\n",
    "        # out_jsonl = json.dumps(response, indent=2)\n",
    "\n",
    "        # # Parse the provided JSON content\n",
    "        # json_content = json.loads(out_jsonl)\n",
    "\n",
    "        # # Extract the 'content' field and parse it as JSON\n",
    "        # json_content_inner = json.loads(json_content['content'])\n",
    "\n",
    "        # Save the output to a file\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            json.dump(response, outfile, indent=2)\n",
    "    else:\n",
    "        print(f'File {file_path} already exists. Skipping.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the long cell...\n",
      "item 0\n",
      "File chunks/chunk_0000.json already exists. Skipping.\n",
      "item 1\n",
      "File chunks/chunk_0001.json already exists. Skipping.\n",
      "item 2\n",
      "File chunks/chunk_0002.json already exists. Skipping.\n",
      "item 3\n",
      "File chunks/chunk_0003.json already exists. Skipping.\n",
      "item 4\n",
      "File chunks/chunk_0004.json already exists. Skipping.\n",
      "item 5\n",
      "File chunks/chunk_0005.json already exists. Skipping.\n",
      "item 6\n",
      "File chunks/chunk_0006.json already exists. Skipping.\n",
      "item 7\n",
      "File chunks/chunk_0007.json already exists. Skipping.\n",
      "item 8\n",
      "File chunks/chunk_0008.json already exists. Skipping.\n",
      "item 9\n",
      "File chunks/chunk_0009.json already exists. Skipping.\n",
      "item 10\n",
      "File chunks/chunk_0010.json already exists. Skipping.\n",
      "item 11\n",
      "File chunks/chunk_0011.json already exists. Skipping.\n",
      "item 12\n",
      "File chunks/chunk_0012.json already exists. Skipping.\n",
      "item 13\n",
      "File chunks/chunk_0013.json already exists. Skipping.\n",
      "item 14\n",
      "File chunks/chunk_0014.json already exists. Skipping.\n",
      "item 15\n",
      "File chunks/chunk_0015.json already exists. Skipping.\n",
      "item 16\n",
      "File chunks/chunk_0016.json already exists. Skipping.\n",
      "item 17\n",
      "File chunks/chunk_0017.json already exists. Skipping.\n",
      "item 18\n",
      "File chunks/chunk_0018.json already exists. Skipping.\n",
      "item 19\n",
      "File chunks/chunk_0019.json already exists. Skipping.\n",
      "item 20\n",
      "File chunks/chunk_0020.json already exists. Skipping.\n",
      "item 21\n",
      "File chunks/chunk_0021.json already exists. Skipping.\n",
      "item 22\n",
      "File chunks/chunk_0022.json already exists. Skipping.\n",
      "item 23\n",
      "File chunks/chunk_0023.json already exists. Skipping.\n",
      "item 24\n",
      "File chunks/chunk_0024.json already exists. Skipping.\n",
      "item 25\n",
      "File chunks/chunk_0025.json already exists. Skipping.\n",
      "item 26\n",
      "File chunks/chunk_0026.json already exists. Skipping.\n",
      "item 27\n",
      "File chunks/chunk_0027.json already exists. Skipping.\n",
      "item 28\n",
      "File chunks/chunk_0028.json already exists. Skipping.\n",
      "item 29\n",
      "File chunks/chunk_0029.json already exists. Skipping.\n",
      "item 30\n",
      "File chunks/chunk_0030.json already exists. Skipping.\n",
      "item 31\n",
      "File chunks/chunk_0031.json already exists. Skipping.\n",
      "item 32\n",
      "File chunks/chunk_0032.json already exists. Skipping.\n",
      "item 33\n",
      "File chunks/chunk_0033.json already exists. Skipping.\n",
      "item 34\n",
      "File chunks/chunk_0034.json already exists. Skipping.\n",
      "item 35\n",
      "File chunks/chunk_0035.json already exists. Skipping.\n",
      "item 36\n",
      "File chunks/chunk_0036.json already exists. Skipping.\n",
      "item 37\n",
      "File chunks/chunk_0037.json already exists. Skipping.\n",
      "item 38\n",
      "File chunks/chunk_0038.json already exists. Skipping.\n",
      "item 39\n",
      "File chunks/chunk_0039.json already exists. Skipping.\n",
      "item 40\n",
      "File chunks/chunk_0040.json already exists. Skipping.\n",
      "item 41\n",
      "File chunks/chunk_0041.json already exists. Skipping.\n",
      "item 42\n",
      "File chunks/chunk_0042.json already exists. Skipping.\n",
      "item 43\n",
      "File chunks/chunk_0043.json already exists. Skipping.\n",
      "item 44\n",
      "File chunks/chunk_0044.json already exists. Skipping.\n",
      "item 45\n",
      "File chunks/chunk_0045.json already exists. Skipping.\n",
      "item 46\n",
      "File chunks/chunk_0046.json already exists. Skipping.\n",
      "item 47\n",
      "File chunks/chunk_0047.json already exists. Skipping.\n",
      "item 48\n",
      "File chunks/chunk_0048.json already exists. Skipping.\n",
      "item 49\n",
      "File chunks/chunk_0049.json already exists. Skipping.\n",
      "item 50\n",
      "File chunks/chunk_0050.json already exists. Skipping.\n",
      "item 51\n",
      "File chunks/chunk_0051.json already exists. Skipping.\n",
      "item 52\n",
      "File chunks/chunk_0052.json already exists. Skipping.\n",
      "item 53\n",
      "File chunks/chunk_0053.json already exists. Skipping.\n",
      "item 54\n",
      "File chunks/chunk_0054.json already exists. Skipping.\n",
      "item 55\n",
      "File chunks/chunk_0055.json already exists. Skipping.\n",
      "item 56\n",
      "File chunks/chunk_0056.json already exists. Skipping.\n",
      "item 57\n",
      "File chunks/chunk_0057.json already exists. Skipping.\n",
      "item 58\n",
      "File chunks/chunk_0058.json already exists. Skipping.\n",
      "item 59\n",
      "File chunks/chunk_0059.json already exists. Skipping.\n",
      "item 60\n",
      "File chunks/chunk_0060.json already exists. Skipping.\n",
      "item 61\n",
      "File chunks/chunk_0061.json already exists. Skipping.\n",
      "item 62\n",
      "File chunks/chunk_0062.json already exists. Skipping.\n",
      "item 63\n",
      "File chunks/chunk_0063.json already exists. Skipping.\n",
      "item 64\n",
      "File chunks/chunk_0064.json already exists. Skipping.\n",
      "item 65\n",
      "File chunks/chunk_0065.json already exists. Skipping.\n",
      "item 66\n",
      "File chunks/chunk_0066.json already exists. Skipping.\n",
      "item 67\n",
      "File chunks/chunk_0067.json already exists. Skipping.\n",
      "Long cell execution completed.\n"
     ]
    }
   ],
   "source": [
    "# Before the long-running cell, insert this code to ask for user confirmation\n",
    "user_confirmation = input(\"Do you want to start running the gpt cell? (yes/no): \")\n",
    "\n",
    "if user_confirmation.lower() == 'yes':\n",
    "    # Put your long-running code here\n",
    "    # This code will only run if the user enters 'yes' at the prompt\n",
    "    print(\"Running the long cell...\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"item {i}\")\n",
    "        chat_oracle_json(chunk=chunk, indexc=i)\n",
    "        \n",
    "    print(\"Long cell execution completed.\")\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning the procesed chunks at `/chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_content_to_json(input_dir, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all files in the input directory\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Load the dictionary from the file\n",
    "            with open(file_path, \"r\") as input_file:\n",
    "                dictionary = json.load(input_file)\n",
    "            \n",
    "            # Get the content and convert it to JSON\n",
    "            content = dictionary[\"content\"]\n",
    "            content_json = json.dumps(content, indent=4)\n",
    "            \n",
    "            # Parse the provided JSON content\n",
    "            json_content = json.loads(content_json)\n",
    "            \n",
    "            # Extract the 'content' field and parse it as JSON\n",
    "            json_content_inner, end = json.JSONDecoder().raw_decode(json_content)\n",
    "\n",
    "            # Warn if there's more data after the first JSON object\n",
    "            # if end < len(json_content):\n",
    "            #     warnings.warn(f\"Extra data found in file {file} after the first JSON object, which was ignored.\")\n",
    "            \n",
    "            # Construct output file path\n",
    "            output_file_path = os.path.join(output_dir, file)\n",
    "            \n",
    "            # Write the converted content to the output file\n",
    "            with open(output_file_path, \"w\") as output_file:\n",
    "                json.dump(json_content_inner, output_file, indent=4)  # Using indent for pretty-printing\n",
    "\n",
    "# Set the input and output directories\n",
    "input_dir = \"chunks/\"\n",
    "output_dir = \"preprocess/jsons/\"\n",
    "\n",
    "# Call the function\n",
    "convert_content_to_json(input_dir, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
